from gensim.models import Word2Vec

# BPE로 토큰화된 텍스트 데이터
tokenized_text = [['ìĬĪ', 'íį¼', 'íŀĪìĸ´ë¡ľ', 'Ġë²Ħëĵľ', 'ë§¨ìľ¼ë¡ľ', 'Ġíķłë¦¬', 'ìļ°ëĵľ', 'ĠíĨ±', 'ĠìĬ¤íĥĢ', 'ìĹĲ', 'Ġìĺ¬ëŀĲ', 'ì§Ģë§Į', 'Ġì§Ģê¸ĪìĿĢ', 'ĠìŀĬíĺĢ', 'ì§Ħ', 'Ġë°°ìļ°', 'Ġë¦¬ê±´', 'ĠíĨ°ìĬ¨', '.', 'Ġê·¸ëĬĶ', 'Ġê¿Īê³¼', 'ĠëªħìĦ±ìĿĦ', 'ĠëĲĺì°¾ê¸°', 'ĠìľĦíķ´', 'Ġë¸Įë¡ľ', 'ëĵľ', 'ìĽ¨ìĿ´', 'Ġë¬´', 'ëĮĢìĹĲ', 'ĠëıĦìłĦ', 'íķľëĭ¤', '.', 'ĠëĮĢì¤ĳ', 'ê³¼', 'Ġë©Ģ', 'ìĸ´ì§Ģê³ł', ',', 'ĠìŀĳíĴĪ', 'ìľ¼ë¡ľ', 'ĠìĿ¸ìłķ', 'ë°ĽìĿĢ', 'Ġìłģ', 'ĠìĹĨëĬĶ', 'Ġë°°ìļ°', 'ìĹĲê²Į', 'ĠíĺĦìĭ¤ìĿĢ', 'Ġê·¸ìĿĺ', 'ĠìĿ´ìĥģ', 'ê³¼', 'Ġê±°', 'ë¦¬ê°Ģ', 'Ġë©Ģëĭ¤', '.', 'Ġìŀ¬', 'ê¸°ìĹĲ', 'ĠëĮĢíķľ', 'Ġê°ķ', 'ë°ķ', 'ê³¼', 'Ġìĭ¬ê°ģ', 'íķľ', 'ĠìŀĲê¸Ī', 'Ġìķķ', 'ë°ķ', 'ĠìĨįìĹĲ', ',', 'Ġíıī', 'ëĭ¨ìĿ´', 'ĠìĤ¬ëŀĳíķĺëĬĶ', 'Ġì£¼', 'ìĹ°', 'ë°°', 'ìļ°ìĿĺ', 'ĠíĨµìłľë¶Ī', 'ê°Ģ', 'ĠíĸīëıĻ', 'ëĵ¤', ',', 'Ġë¬´', 'ëªħ', 'ë°°', 'ìļ°ìĿĺ', 'Ġë¶ĪìķĪ', 'ê°Ĳ', ',', 'ĠSNS', 'Ġê³Ħ', 'ìłķ', 'ĠíķĺëĤĺ', 'ĠìĹĨëĬĶ', 'ĠìķĦë¹łìĿĺ', 'ĠëıĦìłĦ', 'ìĹĲ', 'Ġëĥī', 'ìĨĮ', 'ìłģìĿ¸', 'Ġë§¤ëĭĪìłĢ', 'ĠëĶ¸', ',', 'ĠìĹ°', 'ê·¹', 'ê³Ħë¥¼', 'Ġì¢Į', 'ì§Ģ', 'ìļ°', 'ì§Ģ', 'ĠíķĺëĬĶ', 'Ġíıī', 'ë¡ł', 'ê°ĢìĿĺ', 'Ġìķħ', 'íıī', 'ĠìĺĪê³ł', 'ê¹Įì§Ģ', '...', 'Ġê³¼ìĹ°', 'Ġë²Ħëĵľ', 'ë§¨', 'Ġë¦¬ê±´', 'ìĿĢ', 'Ġëĭ¤ìĭľ', 'ĠëĤłìķĦ', 'ìĺ¤ë¥¼', 'ĠìĪĺ', 'ĠìŀĪìĿĦê¹Į', '?']]

# Word2Vec 모델 훈련
model = Word2Vec(tokenized_text, vector_size=100, window=5, min_count=1, sg=0)

# 모델 저장 (옵션)
model.save("word2vec_model.model")